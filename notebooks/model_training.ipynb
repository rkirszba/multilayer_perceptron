{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to take advantage of the data exploration process to train a performant model.\n",
    "\n",
    "In addition, we will iterate on some hyperparameters to better fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir)\n",
    "\n",
    "from utils.model import FTMultilayerPerceptron\n",
    "from utils.data_processing.normalizers import FTStandardScaler\n",
    "from utils.data_processing.transform_labels import get_labels, labels_to_numbers\n",
    "from utils.data_processing.one_hot import one_hot_encoder, one_hot_decoder\n",
    "from utils.data_processing.selection import train_dev_split, KFold\n",
    "from utils.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = pd.read_csv('../data.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1      2      3       4       5        6        7       8   \\\n",
       "0    842302  M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001   \n",
       "1    842517  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869   \n",
       "2  84300903  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974   \n",
       "3  84348301  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414   \n",
       "4  84358402  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980   \n",
       "\n",
       "        9   ...     22     23      24      25      26      27      28      29  \\\n",
       "0  0.14710  ...  25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654   \n",
       "1  0.07017  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860   \n",
       "2  0.12790  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430   \n",
       "3  0.10520  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575   \n",
       "4  0.10430  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625   \n",
       "\n",
       "       30       31  \n",
       "0  0.4601  0.11890  \n",
       "1  0.2750  0.08902  \n",
       "2  0.3613  0.08758  \n",
       "3  0.6638  0.17300  \n",
       "4  0.2364  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set a seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set a seed to be able to reproduce those 'randomized' experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick and dirty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first are going to use all the features of the dataset and a simple implementation of the neural network (with two hidden layer as it is mandatory in the subject to set a minimum of 2 layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_orig.drop(columns=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df.iloc[:, 1:]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_labels(df.iloc[:, :1])\n",
    "y = labels_to_numbers(df.iloc[:, :1], labels)\n",
    "y = one_hot_encoder(y, len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the parameters of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_dimensions = [X.shape[0], 2, 2, y.shape[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/30000 - loss: 0.6931471605599453 - val_loss: 0.6928702229794984\n",
      "End of training:\n",
      "epoch 407/30000 - loss: 0.6598207111772613 - val_loss: 0.6700575532770661\n",
      "\n",
      "Fold number 0\n",
      "Cost for training set = 0.6598144153081701\n",
      "Cost for      dev set = 0.6700575532770661\n",
      "\n",
      "Accuracy for training set = 0.6296296296296297\n",
      "Accuracy for      dev set = 0.6071428571428571\n",
      "\n",
      "epoch 0/30000 - loss: 0.6931471605599453 - val_loss: 0.6929572973889001\n",
      "End of training:\n",
      "epoch 203/30000 - loss: 0.6620339396912712 - val_loss: 0.6832804450061875\n",
      "\n",
      "Fold number 1\n",
      "Cost for training set = 0.6619862898853159\n",
      "Cost for      dev set = 0.6832804450061875\n",
      "\n",
      "Accuracy for training set = 0.6335282651072125\n",
      "Accuracy for      dev set = 0.5714285714285714\n",
      "\n",
      "epoch 0/30000 - loss: 0.6931471605599453 - val_loss: 0.6926769100769491\n",
      "End of training:\n",
      "epoch 5947/30000 - loss: 0.6641213346366042 - val_loss: 0.626686604092358\n",
      "\n",
      "Fold number 2\n",
      "Cost for training set = 0.6641213346366042\n",
      "Cost for      dev set = 0.626686604092358\n",
      "\n",
      "Accuracy for training set = 0.6198830409356725\n",
      "Accuracy for      dev set = 0.6964285714285714\n",
      "\n",
      "epoch 0/30000 - loss: 0.6931471605599453 - val_loss: 0.6927500569886615\n",
      "End of training:\n",
      "epoch 5806/30000 - loss: 0.6621824038985474 - val_loss: 0.6435081392467841\n",
      "\n",
      "Fold number 3\n",
      "Cost for training set = 0.6621824038985475\n",
      "Cost for      dev set = 0.6435081392467841\n",
      "\n",
      "Accuracy for training set = 0.6237816764132553\n",
      "Accuracy for      dev set = 0.6607142857142857\n",
      "\n",
      "epoch 0/30000 - loss: 0.6931471605599453 - val_loss: 0.6927500569886615\n",
      "End of training:\n",
      "epoch 5817/30000 - loss: 0.6621824038985475 - val_loss: 0.6435081392467829\n",
      "\n",
      "Fold number 4\n",
      "Cost for training set = 0.6621824038985474\n",
      "Cost for      dev set = 0.6435081392467829\n",
      "\n",
      "Accuracy for training set = 0.6237816764132553\n",
      "Accuracy for      dev set = 0.6607142857142857\n",
      "\n",
      "epoch 0/30000 - loss: 0.6931471605599453 - val_loss: 0.6927127871579208\n",
      "End of training:\n",
      "epoch 5921/30000 - loss: 0.6631599486665766 - val_loss: 0.6349417405986372\n",
      "\n",
      "Fold number 5\n",
      "Cost for training set = 0.6631599486665767\n",
      "Cost for      dev set = 0.6349417405986372\n",
      "\n",
      "Accuracy for training set = 0.621832358674464\n",
      "Accuracy for      dev set = 0.6785714285714286\n",
      "\n",
      "epoch 0/30000 - loss: 0.6931471605599453 - val_loss: 0.6929130638093146\n",
      "End of training:\n",
      "epoch 279/30000 - loss: 0.6604416069885646 - val_loss: 0.6772908912488317\n",
      "\n",
      "Fold number 6\n",
      "Cost for training set = 0.6604194665391249\n",
      "Cost for      dev set = 0.6772908912488317\n",
      "\n",
      "Accuracy for training set = 0.631578947368421\n",
      "Accuracy for      dev set = 0.5892857142857143\n",
      "\n",
      "epoch 0/30000 - loss: 0.6931471605599453 - val_loss: 0.6929130638093146\n",
      "End of training:\n",
      "epoch 279/30000 - loss: 0.6604416069885646 - val_loss: 0.6772908912488317\n",
      "\n",
      "Fold number 7\n",
      "Cost for training set = 0.6604194665391249\n",
      "Cost for      dev set = 0.6772908912488317\n",
      "\n",
      "Accuracy for training set = 0.631578947368421\n",
      "Accuracy for      dev set = 0.5892857142857143\n",
      "\n",
      "epoch 0/30000 - loss: 0.6931471605599453 - val_loss: 0.692788719569172\n",
      "End of training:\n",
      "epoch 5664/30000 - loss: 0.6611886673655666 - val_loss: 0.652386756189752\n",
      "\n",
      "Fold number 8\n",
      "Cost for training set = 0.6611886673655666\n",
      "Cost for      dev set = 0.652386756189752\n",
      "\n",
      "Accuracy for training set = 0.6257309941520468\n",
      "Accuracy for      dev set = 0.6428571428571429\n",
      "\n",
      "epoch 0/30000 - loss: 0.6931471605599454 - val_loss: 0.6929230746949369\n",
      "End of training:\n",
      "epoch 253/30000 - loss: 0.660430884244147 - val_loss: 0.678979457894187\n",
      "\n",
      "Fold number 9\n",
      "Cost for training set = 0.6604018130273096\n",
      "Cost for      dev set = 0.678979457894187\n",
      "\n",
      "Accuracy for training set = 0.6329365079365079\n",
      "Accuracy for      dev set = 0.5846153846153846\n",
      "\n",
      "\n",
      "\n",
      "Mean Cost for training set = 0.6615876209764888\n",
      "Mean Cost for      dev set = 0.6587930618049416\n",
      "\n",
      "Mean Accuracy for training set = 0.6274262043998886\n",
      "Mean Accuracy for      dev set = 0.6281043956043957\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mean_cost_train = 0\n",
    "mean_cost_dev = 0\n",
    "mean_accuracy_train = 0\n",
    "mean_accuracy_dev = 0\n",
    "\n",
    "for i, (X_train, X_dev, y_train, y_dev) in enumerate(KFold(X, y, k, random_state=random_state)):\n",
    "    \n",
    "    scaler = FTStandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_dev = scaler.transform(X_dev)\n",
    "    \n",
    "    batch_size = X_train.shape[1]\n",
    "    model = FTMultilayerPerceptron(nn_dimensions,\\\n",
    "    batch_size=batch_size,\\\n",
    "    random_state=random_state,\\\n",
    "    early_stopping=True,\\\n",
    "    verbose=10000,\\\n",
    "    max_epoch=30000)\n",
    "    \n",
    "    model.fit(X_train, y_train, X_dev=X_dev, y_dev=y_dev)\n",
    "    \n",
    "    cost_train = cross_entropy_cost(y_train, model.predict_probas(X_train))\n",
    "    mean_cost_train += cost_train\n",
    "    cost_dev = cross_entropy_cost(y_dev, model.predict_probas(X_dev))\n",
    "    mean_cost_dev += cost_dev\n",
    "    \n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_dev = model.predict(X_dev)\n",
    "    y_truth_train = one_hot_decoder(y_train)\n",
    "    y_truth_dev = one_hot_decoder(y_dev)\n",
    "    accuracy_train = accuracy(y_truth_train, y_pred_train)\n",
    "    mean_accuracy_train += accuracy_train\n",
    "    accuracy_dev = accuracy(y_truth_dev, y_pred_dev)\n",
    "    mean_accuracy_dev += accuracy_dev\n",
    "    \n",
    "    \n",
    "    print()\n",
    "    print('Fold number ' + str(i))\n",
    "    print('Cost for training set = ' + str(cost_train))\n",
    "    print('Cost for      dev set = ' + str(cost_dev))\n",
    "    print()\n",
    "    print('Accuracy for training set = ' + str(accuracy_train))\n",
    "    print('Accuracy for      dev set = ' + str(accuracy_dev))\n",
    "    print()\n",
    "\n",
    "    \n",
    "mean_cost_train /= k\n",
    "mean_cost_dev /= k\n",
    "mean_accuracy_train /= k\n",
    "mean_accuracy_dev /= k\n",
    "\n",
    "print()\n",
    "print()\n",
    "print('Mean Cost for training set = ' + str(mean_cost_train))\n",
    "print('Mean Cost for      dev set = ' + str(mean_cost_dev))\n",
    "print()\n",
    "print('Mean Accuracy for training set = ' + str(mean_accuracy_train))\n",
    "print('Mean Accuracy for      dev set = ' + str(mean_accuracy_dev))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmmm... I didn't work a lot on the data but such awful results look suspicious... Is this a dying relu problem ?\n",
    "\n",
    "Let's try this time with leaking relu to avoid this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/30000 - loss: 0.7252764759887506 - val_loss: 0.7289629701478995\n",
      "End of training:\n",
      "epoch 6556/30000 - loss: 0.05014150718065007 - val_loss: 0.01753943862347959\n",
      "\n",
      "Fold number 0\n",
      "Cost for training set = 0.05013834561482558\n",
      "Cost for      dev set = 0.01753943862347959\n",
      "\n",
      "Accuracy for training set = 0.9902534113060428\n",
      "Accuracy for      dev set = 1.0\n",
      "\n",
      "epoch 0/30000 - loss: 0.7252231768283781 - val_loss: 0.7275137596021468\n",
      "End of training:\n",
      "epoch 2883/30000 - loss: 0.06810429862105259 - val_loss: 0.08779325484643037\n",
      "\n",
      "Fold number 1\n",
      "Cost for training set = 0.06809727464234808\n",
      "Cost for      dev set = 0.08779325484643037\n",
      "\n",
      "Accuracy for training set = 0.9785575048732943\n",
      "Accuracy for      dev set = 0.9642857142857143\n",
      "\n",
      "epoch 0/30000 - loss: 0.72655029150947 - val_loss: 0.7215084128094712\n",
      "End of training:\n",
      "epoch 3179/30000 - loss: 0.0680357012962475 - val_loss: 0.06723710959446409\n",
      "\n",
      "Fold number 2\n",
      "Cost for training set = 0.0680300977353123\n",
      "Cost for      dev set = 0.06723710959446409\n",
      "\n",
      "Accuracy for training set = 0.9824561403508771\n",
      "Accuracy for      dev set = 0.9821428571428571\n",
      "\n",
      "epoch 0/30000 - loss: 0.7269064211542849 - val_loss: 0.7214792151638655\n"
     ]
    }
   ],
   "source": [
    "mean_cost_train = 0\n",
    "mean_cost_dev = 0\n",
    "mean_accuracy_train = 0\n",
    "mean_accuracy_dev = 0\n",
    "\n",
    "for i, (X_train, X_dev, y_train, y_dev) in enumerate(KFold(X, y, k, random_state=random_state)):\n",
    "    \n",
    "    scaler = FTStandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_dev = scaler.transform(X_dev)\n",
    "    \n",
    "    batch_size = X_train.shape[1]\n",
    "    model = FTMultilayerPerceptron(nn_dimensions,\\\n",
    "    hidden_activation='lrelu',\\\n",
    "    batch_size=batch_size,\\\n",
    "    random_state=random_state,\\\n",
    "    early_stopping=True,\\\n",
    "    verbose=10000,\\\n",
    "    max_epoch=30000)\n",
    "    \n",
    "    model.fit(X_train, y_train, X_dev=X_dev, y_dev=y_dev)\n",
    "    \n",
    "    cost_train = cross_entropy_cost(y_train, model.predict_probas(X_train))\n",
    "    mean_cost_train += cost_train\n",
    "    cost_dev = cross_entropy_cost(y_dev, model.predict_probas(X_dev))\n",
    "    mean_cost_dev += cost_dev\n",
    "    \n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_dev = model.predict(X_dev)\n",
    "    y_truth_train = one_hot_decoder(y_train)\n",
    "    y_truth_dev = one_hot_decoder(y_dev)\n",
    "    accuracy_train = accuracy(y_truth_train, y_pred_train)\n",
    "    mean_accuracy_train += accuracy_train\n",
    "    accuracy_dev = accuracy(y_truth_dev, y_pred_dev)\n",
    "    mean_accuracy_dev += accuracy_dev\n",
    "    \n",
    "    \n",
    "    print()\n",
    "    print('Fold number ' + str(i))\n",
    "    print('Cost for training set = ' + str(cost_train))\n",
    "    print('Cost for      dev set = ' + str(cost_dev))\n",
    "    print()\n",
    "    print('Accuracy for training set = ' + str(accuracy_train))\n",
    "    print('Accuracy for      dev set = ' + str(accuracy_dev))\n",
    "    print()\n",
    "\n",
    "    \n",
    "mean_cost_train /= k\n",
    "mean_cost_dev /= k\n",
    "mean_accuracy_train /= k\n",
    "mean_accuracy_dev /= k\n",
    "\n",
    "print()\n",
    "print()\n",
    "print('Mean Cost for training set = ' + str(mean_cost_train))\n",
    "print('Mean Cost for      dev set = ' + str(mean_cost_dev))\n",
    "print()\n",
    "print('Mean Accuracy for training set = ' + str(mean_accuracy_train))\n",
    "print('Mean Accuracy for      dev set = ' + str(mean_accuracy_dev))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's better :) With a not so deep neural network, and no work on the data (except normalization), the accuracy is pretty descent ! But let's try to do better !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decrease the bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_dimensions = [X_train.shape[0], 20, 20, 20, y_train.shape[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/100000 - loss: 0.9351147094054832 - val_loss: 0.8931816892990122\n",
      "End of training:\n",
      "epoch 3083/100000 - loss: 0.0254270387935981 - val_loss: 0.03462181734367178\n",
      "\n",
      "Fold number 0\n",
      "Cost for training set = 0.0254184445426214\n",
      "Cost for      dev set = 0.03462181734367178\n",
      "\n",
      "Accuracy for training set = 0.9902534113060428\n",
      "Accuracy for      dev set = 0.9821428571428571\n",
      "\n",
      "epoch 0/100000 - loss: 0.9310020225468737 - val_loss: 0.9308833501332077\n",
      "End of training:\n",
      "epoch 3665/100000 - loss: 0.02168985200509469 - val_loss: 0.07447561441921886\n",
      "\n",
      "Fold number 1\n",
      "Cost for training set = 0.021684509691213688\n",
      "Cost for      dev set = 0.07447561441921886\n",
      "\n",
      "Accuracy for training set = 0.9922027290448343\n",
      "Accuracy for      dev set = 0.9464285714285714\n",
      "\n",
      "epoch 0/100000 - loss: 0.9406947156723722 - val_loss: 0.8725817169754615\n",
      "End of training:\n",
      "epoch 2643/100000 - loss: 0.029979643703953833 - val_loss: 0.03783544243830451\n",
      "\n",
      "Fold number 2\n",
      "Cost for training set = 0.029969189465917118\n",
      "Cost for      dev set = 0.03783544243830451\n",
      "\n",
      "Accuracy for training set = 0.9902534113060428\n",
      "Accuracy for      dev set = 0.9821428571428571\n",
      "\n",
      "epoch 0/100000 - loss: 0.9466384121198018 - val_loss: 0.8211745509504806\n",
      "End of training:\n",
      "epoch 6761/100000 - loss: 0.007468878621452218 - val_loss: 0.002917927606737312\n",
      "\n",
      "Fold number 3\n",
      "Cost for training set = 0.007466038345565146\n",
      "Cost for      dev set = 0.002917927606737312\n",
      "\n",
      "Accuracy for training set = 1.0\n",
      "Accuracy for      dev set = 1.0\n",
      "\n",
      "epoch 0/100000 - loss: 0.9280910548401847 - val_loss: 1.005208529659518\n",
      "End of training:\n",
      "epoch 1206/100000 - loss: 0.05099168722567649 - val_loss: 0.07098478827131334\n",
      "\n",
      "Fold number 4\n",
      "Cost for training set = 0.05096169004809865\n",
      "Cost for      dev set = 0.07098478827131334\n",
      "\n",
      "Accuracy for training set = 0.9844054580896686\n",
      "Accuracy for      dev set = 0.9642857142857143\n",
      "\n",
      "epoch 0/100000 - loss: 0.9336768679896787 - val_loss: 0.9497949914154378\n",
      "End of training:\n",
      "epoch 1000/100000 - loss: 0.06174982733379874 - val_loss: 0.05739378475784149\n",
      "\n",
      "Fold number 5\n",
      "Cost for training set = 0.06171004912441238\n",
      "Cost for      dev set = 0.05739378475784149\n",
      "\n",
      "Accuracy for training set = 0.9844054580896686\n",
      "Accuracy for      dev set = 0.9821428571428571\n",
      "\n",
      "epoch 0/100000 - loss: 0.9484615702907444 - val_loss: 0.8337121116561053\n",
      "End of training:\n",
      "epoch 321/100000 - loss: 0.09864581720277157 - val_loss: 0.225459148554941\n",
      "\n",
      "Fold number 6\n",
      "Cost for training set = 0.09846001457457934\n",
      "Cost for      dev set = 0.225459148554941\n",
      "\n",
      "Accuracy for training set = 0.9473684210526315\n",
      "Accuracy for      dev set = 0.9285714285714286\n",
      "\n",
      "epoch 0/100000 - loss: 0.9403798560123926 - val_loss: 0.9047311083313726\n",
      "End of training:\n",
      "epoch 1074/100000 - loss: 0.058393438617006306 - val_loss: 0.04393499397734192\n",
      "\n",
      "Fold number 7\n",
      "Cost for training set = 0.058352569351931415\n",
      "Cost for      dev set = 0.04393499397734192\n",
      "\n",
      "Accuracy for training set = 0.9844054580896686\n",
      "Accuracy for      dev set = 0.9821428571428571\n",
      "\n",
      "epoch 0/100000 - loss: 0.9398731491323463 - val_loss: 0.8431031274465001\n",
      "End of training:\n",
      "epoch 5702/100000 - loss: 0.009870341445851711 - val_loss: 0.009919351017046702\n",
      "\n",
      "Fold number 8\n",
      "Cost for training set = 0.009867049242682637\n",
      "Cost for      dev set = 0.009919351017046702\n",
      "\n",
      "Accuracy for training set = 1.0\n",
      "Accuracy for      dev set = 1.0\n",
      "\n",
      "epoch 0/100000 - loss: 0.9336165501657951 - val_loss: 0.893838324239536\n",
      "End of training:\n",
      "epoch 1785/100000 - loss: 0.036033519896169125 - val_loss: 0.06800741514092407\n",
      "\n",
      "Fold number 9\n",
      "Cost for training set = 0.03601498054807688\n",
      "Cost for      dev set = 0.06800741514092407\n",
      "\n",
      "Accuracy for training set = 0.9900793650793651\n",
      "Accuracy for      dev set = 0.9692307692307692\n",
      "\n",
      "\n",
      "\n",
      "Mean Cost for training set = 0.03999045349350987\n",
      "Mean Cost for      dev set = 0.0625550283527341\n",
      "\n",
      "Mean Accuracy for training set = 0.9863373712057923\n",
      "Mean Accuracy for      dev set = 0.9737087912087912\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mean_cost_train = 0\n",
    "mean_cost_dev = 0\n",
    "mean_accuracy_train = 0\n",
    "mean_accuracy_dev = 0\n",
    "\n",
    "for i, (X_train, X_dev, y_train, y_dev) in enumerate(KFold(X, y, k, random_state=random_state)):\n",
    "      \n",
    "    scaler = FTStandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_dev = scaler.transform(X_dev)\n",
    "    \n",
    "    batch_size = X_train.shape[1]\n",
    "    model = FTMultilayerPerceptron(nn_dimensions,\\\n",
    "        batch_size=batch_size,\\\n",
    "        random_state=random_state,\\\n",
    "        early_stopping=True,\\\n",
    "    #    l2_reg = True,\\\n",
    "    #    lambd=1.5,\\\n",
    "        max_epoch=100000,\\\n",
    "        verbose=10000)\n",
    "    \n",
    "    model.fit(X_train, y_train, X_dev=X_dev, y_dev=y_dev)\n",
    "    \n",
    "    cost_train = cross_entropy_cost(y_train, model.predict_probas(X_train))\n",
    "    mean_cost_train += cost_train\n",
    "    cost_dev = cross_entropy_cost(y_dev, model.predict_probas(X_dev))\n",
    "    mean_cost_dev += cost_dev\n",
    "    \n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_dev = model.predict(X_dev)\n",
    "    y_truth_train = one_hot_decoder(y_train)\n",
    "    y_truth_dev = one_hot_decoder(y_dev)\n",
    "    accuracy_train = accuracy(y_truth_train, y_pred_train)\n",
    "    mean_accuracy_train += accuracy_train\n",
    "    accuracy_dev = accuracy(y_truth_dev, y_pred_dev)\n",
    "    mean_accuracy_dev += accuracy_dev\n",
    "    \n",
    "    print()\n",
    "    print('Fold number ' + str(i))\n",
    "    print('Cost for training set = ' + str(cost_train))\n",
    "    print('Cost for      dev set = ' + str(cost_dev))\n",
    "    print()\n",
    "    print('Accuracy for training set = ' + str(accuracy_train))\n",
    "    print('Accuracy for      dev set = ' + str(accuracy_dev))\n",
    "    print()\n",
    "\n",
    "    \n",
    "mean_cost_train /= k\n",
    "mean_cost_dev /= k\n",
    "mean_accuracy_train /= k\n",
    "mean_accuracy_dev /= k\n",
    "\n",
    "print()\n",
    "print()\n",
    "print('Mean Cost for training set = ' + str(mean_cost_train))\n",
    "print('Mean Cost for      dev set = ' + str(mean_cost_dev))\n",
    "print()\n",
    "print('Mean Accuracy for training set = ' + str(mean_accuracy_train))\n",
    "print('Mean Accuracy for      dev set = ' + str(mean_accuracy_dev))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With another optimization function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
