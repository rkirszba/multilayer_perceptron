{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to take advantage of the data exploration process to train a performant model.\n",
    "\n",
    "In addition, we will iterate on some hyperparameters to better fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir)\n",
    "\n",
    "from utils.model import FTMultilayerPerceptron\n",
    "from utils.data_processing.normalizers import FTStandardScaler\n",
    "from utils.data_processing.transform_labels import get_labels, labels_to_numbers\n",
    "from utils.data_processing.one_hot import one_hot_encoder, one_hot_decoder\n",
    "from utils.data_processing.selection import train_dev_split, KFold\n",
    "from utils.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = pd.read_csv('../data.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1      2      3       4       5        6        7       8   \\\n",
       "0    842302  M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001   \n",
       "1    842517  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869   \n",
       "2  84300903  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974   \n",
       "3  84348301  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414   \n",
       "4  84358402  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980   \n",
       "\n",
       "        9   ...     22     23      24      25      26      27      28      29  \\\n",
       "0  0.14710  ...  25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654   \n",
       "1  0.07017  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860   \n",
       "2  0.12790  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430   \n",
       "3  0.10520  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575   \n",
       "4  0.10430  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625   \n",
       "\n",
       "       30       31  \n",
       "0  0.4601  0.11890  \n",
       "1  0.2750  0.08902  \n",
       "2  0.3613  0.08758  \n",
       "3  0.6638  0.17300  \n",
       "4  0.2364  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set a seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set a seed to be able to reproduce those 'randomized' experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick and dirty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first are going to use all the features of the dataset and a simple implementation of the neural network (with two hidden layer as it is mandatory in the subject to set a minimum of 2 layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_orig.drop(columns=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df.iloc[:, 1:]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_labels(df.iloc[:, :1])\n",
    "y = labels_to_numbers(df.iloc[:, :1], labels)\n",
    "y = one_hot_encoder(y, len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the parameters of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_dimensions = [X.shape[0], 2, 2, y.shape[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/30000 - loss: 0.6931471605599453 - val_loss: 0.6928702229794984\n",
      "End of training:\n",
      "epoch 362/30000 - loss: 0.6601762028127721 - val_loss: 0.6700096531609975\n",
      "\n",
      "Fold number 0\n",
      "Cost for training set = 0.6601665311871561\n",
      "Cost for      dev set = 0.6700096531609975\n",
      "\n",
      "Accuracy for training set = 0.6296296296296297\n",
      "Accuracy for      dev set = 0.6071428571428571\n",
      "\n",
      "epoch 0/30000 - loss: 0.6931471605599453 - val_loss: 0.6929572973889001\n",
      "End of training:\n",
      "epoch 158/30000 - loss: 0.6647396126798509 - val_loss: 0.6829127194904402\n",
      "\n",
      "Fold number 1\n",
      "Cost for training set = 0.6646656387539777\n",
      "Cost for      dev set = 0.6829127194904402\n",
      "\n",
      "Accuracy for training set = 0.6335282651072125\n",
      "Accuracy for      dev set = 0.5714285714285714\n",
      "\n",
      "epoch 0/30000 - loss: 0.6931471605599453 - val_loss: 0.6926769100769491\n",
      "End of training:\n",
      "epoch 6266/30000 - loss: 0.6641213346366043 - val_loss: 0.6266866040923408\n",
      "\n",
      "Fold number 2\n",
      "Cost for training set = 0.6641213346366043\n",
      "Cost for      dev set = 0.6266866040923408\n",
      "\n",
      "Accuracy for training set = 0.6198830409356725\n",
      "Accuracy for      dev set = 0.6964285714285714\n",
      "\n",
      "epoch 0/30000 - loss: 0.6931471605599453 - val_loss: 0.6927500569886615\n",
      "End of training:\n",
      "epoch 6131/30000 - loss: 0.6621824038985475 - val_loss: 0.6435081392467651\n",
      "\n",
      "Fold number 3\n",
      "Cost for training set = 0.6621824038985474\n",
      "Cost for      dev set = 0.6435081392467651\n",
      "\n",
      "Accuracy for training set = 0.6237816764132553\n",
      "Accuracy for      dev set = 0.6607142857142857\n",
      "\n",
      "epoch 0/30000 - loss: 0.6931471605599453 - val_loss: 0.6927500569886615\n",
      "End of training:\n",
      "epoch 6131/30000 - loss: 0.6621824038985474 - val_loss: 0.6435081392467651\n",
      "\n",
      "Fold number 4\n",
      "Cost for training set = 0.6621824038985474\n",
      "Cost for      dev set = 0.6435081392467651\n",
      "\n",
      "Accuracy for training set = 0.6237816764132553\n",
      "Accuracy for      dev set = 0.6607142857142857\n",
      "\n",
      "epoch 0/30000 - loss: 0.6931471605599453 - val_loss: 0.6927127871579208\n",
      "End of training:\n",
      "epoch 6226/30000 - loss: 0.6631599486665765 - val_loss: 0.6349417405986216\n",
      "\n",
      "Fold number 5\n",
      "Cost for training set = 0.6631599486665767\n",
      "Cost for      dev set = 0.6349417405986216\n",
      "\n",
      "Accuracy for training set = 0.621832358674464\n",
      "Accuracy for      dev set = 0.6785714285714286\n",
      "\n",
      "epoch 0/30000 - loss: 0.6931471605599453 - val_loss: 0.6929130638093146\n",
      "End of training:\n",
      "epoch 234/30000 - loss: 0.6616955381430784 - val_loss: 0.6771198928475642\n",
      "\n",
      "Fold number 6\n",
      "Cost for training set = 0.6616613321728402\n",
      "Cost for      dev set = 0.6771198928475642\n",
      "\n",
      "Accuracy for training set = 0.631578947368421\n",
      "Accuracy for      dev set = 0.5892857142857143\n",
      "\n",
      "epoch 0/30000 - loss: 0.6931471605599453 - val_loss: 0.6929130638093146\n",
      "End of training:\n",
      "epoch 234/30000 - loss: 0.6616955381430784 - val_loss: 0.6771198928475642\n",
      "\n",
      "Fold number 7\n",
      "Cost for training set = 0.6616613321728402\n",
      "Cost for      dev set = 0.6771198928475642\n",
      "\n",
      "Accuracy for training set = 0.631578947368421\n",
      "Accuracy for      dev set = 0.5892857142857143\n",
      "\n",
      "epoch 0/30000 - loss: 0.6931471605599453 - val_loss: 0.692788719569172\n",
      "End of training:\n",
      "epoch 5829/30000 - loss: 0.6611886673655667 - val_loss: 0.6523867561897395\n",
      "\n",
      "Fold number 8\n",
      "Cost for training set = 0.6611886673655668\n",
      "Cost for      dev set = 0.6523867561897395\n",
      "\n",
      "Accuracy for training set = 0.6257309941520468\n",
      "Accuracy for      dev set = 0.6428571428571429\n",
      "\n",
      "epoch 0/30000 - loss: 0.6931471605599454 - val_loss: 0.6929230746949369\n",
      "End of training:\n",
      "epoch 208/30000 - loss: 0.6620785296332903 - val_loss: 0.6787608609968079\n",
      "\n",
      "Fold number 9\n",
      "Cost for training set = 0.6620335548390579\n",
      "Cost for      dev set = 0.6787608609968079\n",
      "\n",
      "Accuracy for training set = 0.6329365079365079\n",
      "Accuracy for      dev set = 0.5846153846153846\n",
      "\n",
      "\n",
      "\n",
      "Mean Cost for training set = 0.6623023147591715\n",
      "Mean Cost for      dev set = 0.6586954398717605\n",
      "\n",
      "Mean Accuracy for training set = 0.6274262043998886\n",
      "Mean Accuracy for      dev set = 0.6281043956043957\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mean_cost_train = 0\n",
    "mean_cost_dev = 0\n",
    "mean_accuracy_train = 0\n",
    "mean_accuracy_dev = 0\n",
    "\n",
    "for i, (X_train, X_dev, y_train, y_dev) in enumerate(KFold(X, y, k, random_state=random_state)):\n",
    "    \n",
    "    scaler = FTStandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_dev = scaler.transform(X_dev)\n",
    "    \n",
    "    batch_size = X_train.shape[1]\n",
    "    model = FTMultilayerPerceptron(nn_dimensions,\\\n",
    "    batch_size=batch_size,\\\n",
    "    random_state=random_state,\\\n",
    "    early_stopping=True,\\\n",
    "    patience=5,\\\n",
    "    verbose=10000,\\\n",
    "    max_epoch=30000)\n",
    "    \n",
    "    model.fit(X_train, y_train, X_dev=X_dev, y_dev=y_dev)\n",
    "    \n",
    "    cost_train = cross_entropy_cost(y_train, model.predict_probas(X_train))\n",
    "    mean_cost_train += cost_train\n",
    "    cost_dev = cross_entropy_cost(y_dev, model.predict_probas(X_dev))\n",
    "    mean_cost_dev += cost_dev\n",
    "    \n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_dev = model.predict(X_dev)\n",
    "    y_truth_train = one_hot_decoder(y_train)\n",
    "    y_truth_dev = one_hot_decoder(y_dev)\n",
    "    accuracy_train = accuracy(y_truth_train, y_pred_train)\n",
    "    mean_accuracy_train += accuracy_train\n",
    "    accuracy_dev = accuracy(y_truth_dev, y_pred_dev)\n",
    "    mean_accuracy_dev += accuracy_dev\n",
    "    \n",
    "    \n",
    "    print()\n",
    "    print('Fold number ' + str(i))\n",
    "    print('Cost for training set = ' + str(cost_train))\n",
    "    print('Cost for      dev set = ' + str(cost_dev))\n",
    "    print()\n",
    "    print('Accuracy for training set = ' + str(accuracy_train))\n",
    "    print('Accuracy for      dev set = ' + str(accuracy_dev))\n",
    "    print()\n",
    "\n",
    "    \n",
    "mean_cost_train /= k\n",
    "mean_cost_dev /= k\n",
    "mean_accuracy_train /= k\n",
    "mean_accuracy_dev /= k\n",
    "\n",
    "print()\n",
    "print()\n",
    "print('Mean Cost for training set = ' + str(mean_cost_train))\n",
    "print('Mean Cost for      dev set = ' + str(mean_cost_dev))\n",
    "print()\n",
    "print('Mean Accuracy for training set = ' + str(mean_accuracy_train))\n",
    "print('Mean Accuracy for      dev set = ' + str(mean_accuracy_dev))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmmm... I didn't work a lot on the data but such awful results look suspicious... Is this a dying relu problem ?\n",
    "\n",
    "Let's try this time with tanh to avoid this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/30000 - loss: 1.0397859541150594 - val_loss: 1.0703188862491215\n",
      "End of training:\n",
      "epoch 6728/30000 - loss: 0.04155336658051457 - val_loss: 0.014590099648048157\n",
      "\n",
      "Fold number 0\n",
      "Cost for training set = 0.04155122370556702\n",
      "Cost for      dev set = 0.014590099648048157\n",
      "\n",
      "Accuracy for training set = 0.9922027290448343\n",
      "Accuracy for      dev set = 1.0\n",
      "\n",
      "epoch 0/30000 - loss: 1.0417325249590372 - val_loss: 0.9911456989764061\n",
      "End of training:\n",
      "epoch 1406/30000 - loss: 0.06558954125833209 - val_loss: 0.09379850086242773\n",
      "\n",
      "Fold number 1\n",
      "Cost for training set = 0.06556844812227397\n",
      "Cost for      dev set = 0.09379850086242773\n",
      "\n",
      "Accuracy for training set = 0.9844054580896686\n",
      "Accuracy for      dev set = 0.9642857142857143\n",
      "\n",
      "epoch 0/30000 - loss: 1.0466539062125946 - val_loss: 1.0067427543816392\n",
      "End of training:\n",
      "epoch 1366/30000 - loss: 0.06681464547220436 - val_loss: 0.11039817795382167\n",
      "\n",
      "Fold number 2\n",
      "Cost for training set = 0.06679016809196853\n",
      "Cost for      dev set = 0.11039817795382167\n",
      "\n",
      "Accuracy for training set = 0.98635477582846\n",
      "Accuracy for      dev set = 0.9642857142857143\n",
      "\n",
      "epoch 0/30000 - loss: 1.0492978207494292 - val_loss: 1.0203325006012465\n",
      "End of training:\n",
      "epoch 3392/30000 - loss: 0.05695592970721002 - val_loss: 0.03389691663092342\n",
      "\n",
      "Fold number 3\n",
      "Cost for training set = 0.05695026676281153\n",
      "Cost for      dev set = 0.03389691663092342\n",
      "\n",
      "Accuracy for training set = 0.98635477582846\n",
      "Accuracy for      dev set = 0.9821428571428571\n",
      "\n",
      "epoch 0/30000 - loss: 1.0381697543337607 - val_loss: 1.0799304323648082\n",
      "End of training:\n",
      "epoch 2064/30000 - loss: 0.054292311328979714 - val_loss: 0.08291154176060099\n",
      "\n",
      "Fold number 4\n",
      "Cost for training set = 0.05428232444582601\n",
      "Cost for      dev set = 0.08291154176060099\n",
      "\n",
      "Accuracy for training set = 0.9883040935672515\n",
      "Accuracy for      dev set = 0.9821428571428571\n",
      "\n",
      "epoch 0/30000 - loss: 1.050998734077685 - val_loss: 0.9637084782616614\n",
      "epoch 10000/30000 - loss: 0.03018171385920701 - val_loss: 0.02881552408620653\n",
      "End of training:\n",
      "epoch 15980/30000 - loss: 0.01839916213768791 - val_loss: 0.024892890168263813\n",
      "\n",
      "Fold number 5\n",
      "Cost for training set = 0.018397882539102642\n",
      "Cost for      dev set = 0.024892890168263813\n",
      "\n",
      "Accuracy for training set = 0.9941520467836257\n",
      "Accuracy for      dev set = 0.9821428571428571\n",
      "\n",
      "epoch 0/30000 - loss: 1.0448042951236598 - val_loss: 1.0636597690633447\n",
      "End of training:\n",
      "epoch 717/30000 - loss: 0.08095651931447959 - val_loss: 0.2461567932288165\n",
      "\n",
      "Fold number 6\n",
      "Cost for training set = 0.08081494550679517\n",
      "Cost for      dev set = 0.2461567932288165\n",
      "\n",
      "Accuracy for training set = 0.9902534113060428\n",
      "Accuracy for      dev set = 0.9285714285714286\n",
      "\n",
      "epoch 0/30000 - loss: 1.0467330688898362 - val_loss: 1.0081797803633332\n",
      "epoch 10000/30000 - loss: 0.03364925218727651 - val_loss: 0.06605699612287753\n",
      "End of training:\n",
      "epoch 15779/30000 - loss: 0.01893381007009366 - val_loss: 0.04970434041757264\n",
      "\n",
      "Fold number 7\n",
      "Cost for training set = 0.01893224453677437\n",
      "Cost for      dev set = 0.04970434041757264\n",
      "\n",
      "Accuracy for training set = 0.9941520467836257\n",
      "Accuracy for      dev set = 0.9821428571428571\n",
      "\n",
      "epoch 0/30000 - loss: 1.042067345024673 - val_loss: 1.095227083280452\n",
      "End of training:\n",
      "epoch 7502/30000 - loss: 0.03984647916352657 - val_loss: 0.006490452780750927\n",
      "\n",
      "Fold number 8\n",
      "Cost for training set = 0.03984465897427163\n",
      "Cost for      dev set = 0.006490452780750927\n",
      "\n",
      "Accuracy for training set = 0.9922027290448343\n",
      "Accuracy for      dev set = 1.0\n",
      "\n",
      "epoch 0/30000 - loss: 1.0424435454302075 - val_loss: 1.027321593644443\n",
      "End of training:\n",
      "epoch 6209/30000 - loss: 0.042895903138552766 - val_loss: 0.022950712131236426\n",
      "\n",
      "Fold number 9\n",
      "Cost for training set = 0.04289278151929366\n",
      "Cost for      dev set = 0.022950712131236426\n",
      "\n",
      "Accuracy for training set = 0.9920634920634921\n",
      "Accuracy for      dev set = 0.9846153846153847\n",
      "\n",
      "\n",
      "\n",
      "Mean Cost for training set = 0.048602494420468455\n",
      "Mean Cost for      dev set = 0.06857904255824623\n",
      "\n",
      "Mean Accuracy for training set = 0.9900445558340294\n",
      "Mean Accuracy for      dev set = 0.9770329670329669\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mean_cost_train = 0\n",
    "mean_cost_dev = 0\n",
    "mean_accuracy_train = 0\n",
    "mean_accuracy_dev = 0\n",
    "\n",
    "for i, (X_train, X_dev, y_train, y_dev) in enumerate(KFold(X, y, k, random_state=random_state)):\n",
    "    \n",
    "    scaler = FTStandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_dev = scaler.transform(X_dev)\n",
    "    \n",
    "    batch_size = X_train.shape[1]\n",
    "    model = FTMultilayerPerceptron(nn_dimensions,\\\n",
    "    hidden_activation='tanh',\\\n",
    "    batch_size=batch_size,\\\n",
    "    random_state=random_state,\\\n",
    "    early_stopping=True,\\\n",
    "    patience=5,\\\n",
    "    verbose=10000,\\\n",
    "    max_epoch=30000)\n",
    "    \n",
    "    model.fit(X_train, y_train, X_dev=X_dev, y_dev=y_dev)\n",
    "    \n",
    "    cost_train = cross_entropy_cost(y_train, model.predict_probas(X_train))\n",
    "    mean_cost_train += cost_train\n",
    "    cost_dev = cross_entropy_cost(y_dev, model.predict_probas(X_dev))\n",
    "    mean_cost_dev += cost_dev\n",
    "    \n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_dev = model.predict(X_dev)\n",
    "    y_truth_train = one_hot_decoder(y_train)\n",
    "    y_truth_dev = one_hot_decoder(y_dev)\n",
    "    accuracy_train = accuracy(y_truth_train, y_pred_train)\n",
    "    mean_accuracy_train += accuracy_train\n",
    "    accuracy_dev = accuracy(y_truth_dev, y_pred_dev)\n",
    "    mean_accuracy_dev += accuracy_dev\n",
    "    \n",
    "    \n",
    "    print()\n",
    "    print('Fold number ' + str(i))\n",
    "    print('Cost for training set = ' + str(cost_train))\n",
    "    print('Cost for      dev set = ' + str(cost_dev))\n",
    "    print()\n",
    "    print('Accuracy for training set = ' + str(accuracy_train))\n",
    "    print('Accuracy for      dev set = ' + str(accuracy_dev))\n",
    "    print()\n",
    "\n",
    "    \n",
    "mean_cost_train /= k\n",
    "mean_cost_dev /= k\n",
    "mean_accuracy_train /= k\n",
    "mean_accuracy_dev /= k\n",
    "\n",
    "print()\n",
    "print()\n",
    "print('Mean Cost for training set = ' + str(mean_cost_train))\n",
    "print('Mean Cost for      dev set = ' + str(mean_cost_dev))\n",
    "print()\n",
    "print('Mean Accuracy for training set = ' + str(mean_accuracy_train))\n",
    "print('Mean Accuracy for      dev set = ' + str(mean_accuracy_dev))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's better :) With a not so deep neural network, and no work on the data (except normalization), the accuracy is pretty descent ! But let's try to do better !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decrease the bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_dimensions = [X_train.shape[0], 7, 7, y_train.shape[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/100000 - loss: 0.99740596344597 - val_loss: 0.9874425480493414\n",
      "epoch 10000/100000 - loss: 0.03306305109808842 - val_loss: 0.011264777917208946\n",
      "epoch 20000/100000 - loss: 0.015409171971058651 - val_loss: 0.008873685199254258\n",
      "epoch 30000/100000 - loss: 0.006575954933323627 - val_loss: 0.006347753614584037\n",
      "End of training:\n",
      "epoch 31088/100000 - loss: 0.005987313424912897 - val_loss: 0.006304914635190286\n",
      "\n",
      "Fold number 0\n",
      "Cost for training set = 0.005986792670496652\n",
      "Cost for      dev set = 0.006304914635190286\n",
      "\n",
      "Accuracy for training set = 0.9980506822612085\n",
      "Accuracy for      dev set = 1.0\n",
      "\n",
      "epoch 0/100000 - loss: 0.99520423704617 - val_loss: 1.0002616336176215\n",
      "End of training:\n",
      "epoch 2504/100000 - loss: 0.05585143732219983 - val_loss: 0.11630170845898197\n",
      "\n",
      "Fold number 1\n",
      "Cost for training set = 0.05584304270000636\n",
      "Cost for      dev set = 0.11630170845898197\n",
      "\n",
      "Accuracy for training set = 0.98635477582846\n",
      "Accuracy for      dev set = 0.9464285714285714\n",
      "\n",
      "epoch 0/100000 - loss: 1.000447719023107 - val_loss: 0.947100251366208\n",
      "End of training:\n",
      "epoch 1693/100000 - loss: 0.06651443176881337 - val_loss: 0.06076063169126682\n",
      "\n",
      "Fold number 2\n",
      "Cost for training set = 0.06649850666760798\n",
      "Cost for      dev set = 0.06076063169126682\n",
      "\n",
      "Accuracy for training set = 0.9824561403508771\n",
      "Accuracy for      dev set = 0.9821428571428571\n",
      "\n",
      "epoch 0/100000 - loss: 0.9957928185244058 - val_loss: 0.994750344417935\n",
      "End of training:\n",
      "epoch 4570/100000 - loss: 0.050132324032279484 - val_loss: 0.09334026308007864\n",
      "\n",
      "Fold number 3\n",
      "Cost for training set = 0.050128253970552525\n",
      "Cost for      dev set = 0.09334026308007864\n",
      "\n",
      "Accuracy for training set = 0.98635477582846\n",
      "Accuracy for      dev set = 0.9821428571428571\n",
      "\n",
      "epoch 0/100000 - loss: 0.9983341175182326 - val_loss: 0.9884617731883196\n",
      "End of training:\n",
      "epoch 2725/100000 - loss: 0.053425373100049446 - val_loss: 0.08335005222851462\n",
      "\n",
      "Fold number 4\n",
      "Cost for training set = 0.05341837830441639\n",
      "Cost for      dev set = 0.08335005222851462\n",
      "\n",
      "Accuracy for training set = 0.9844054580896686\n",
      "Accuracy for      dev set = 0.9821428571428571\n",
      "\n",
      "epoch 0/100000 - loss: 0.9953661093038998 - val_loss: 0.9873478645897326\n",
      "epoch 10000/100000 - loss: 0.03190859823485665 - val_loss: 0.018415790257938672\n",
      "End of training:\n",
      "epoch 12135/100000 - loss: 0.026197863755228232 - val_loss: 0.017644678639807756\n",
      "\n",
      "Fold number 5\n",
      "Cost for training set = 0.026195444606333804\n",
      "Cost for      dev set = 0.017644678639807756\n",
      "\n",
      "Accuracy for training set = 0.9922027290448343\n",
      "Accuracy for      dev set = 1.0\n",
      "\n",
      "epoch 0/100000 - loss: 1.0035914827303087 - val_loss: 0.9608424613650427\n",
      "End of training:\n",
      "epoch 422/100000 - loss: 0.11891025454476542 - val_loss: 0.24195088194466774\n",
      "\n",
      "Fold number 6\n",
      "Cost for training set = 0.118667289141375\n",
      "Cost for      dev set = 0.24195088194466774\n",
      "\n",
      "Accuracy for training set = 0.9629629629629629\n",
      "Accuracy for      dev set = 0.9285714285714286\n",
      "\n",
      "epoch 0/100000 - loss: 0.9883330216640489 - val_loss: 1.0138350863429224\n",
      "End of training:\n",
      "epoch 959/100000 - loss: 0.08210263021387397 - val_loss: 0.08879504645531101\n",
      "\n",
      "Fold number 7\n",
      "Cost for training set = 0.0820570540412877\n",
      "Cost for      dev set = 0.08879504645531101\n",
      "\n",
      "Accuracy for training set = 0.9805068226120858\n",
      "Accuracy for      dev set = 0.9821428571428571\n",
      "\n",
      "epoch 0/100000 - loss: 0.9988956133376684 - val_loss: 0.9789650896967803\n",
      "epoch 10000/100000 - loss: 0.03106369657986015 - val_loss: 0.01208165324877767\n",
      "End of training:\n",
      "epoch 14987/100000 - loss: 0.02138409775762813 - val_loss: 0.010310321091551838\n",
      "\n",
      "Fold number 8\n",
      "Cost for training set = 0.02138271647223822\n",
      "Cost for      dev set = 0.010310321091551838\n",
      "\n",
      "Accuracy for training set = 0.9961013645224172\n",
      "Accuracy for      dev set = 1.0\n",
      "\n",
      "epoch 0/100000 - loss: 0.9944765583490468 - val_loss: 0.9694324172776579\n",
      "End of training:\n",
      "epoch 4849/100000 - loss: 0.04810938586824998 - val_loss: 0.02196716869757918\n",
      "\n",
      "Fold number 9\n",
      "Cost for training set = 0.04810472587578276\n",
      "Cost for      dev set = 0.02196716869757918\n",
      "\n",
      "Accuracy for training set = 0.9861111111111112\n",
      "Accuracy for      dev set = 1.0\n",
      "\n",
      "\n",
      "\n",
      "Mean Cost for training set = 0.052828220445009746\n",
      "Mean Cost for      dev set = 0.07407256669229498\n",
      "\n",
      "Mean Accuracy for training set = 0.9855506822612086\n",
      "Mean Accuracy for      dev set = 0.9803571428571427\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mean_cost_train = 0\n",
    "mean_cost_dev = 0\n",
    "mean_accuracy_train = 0\n",
    "mean_accuracy_dev = 0\n",
    "\n",
    "for i, (X_train, X_dev, y_train, y_dev) in enumerate(KFold(X, y, k, random_state=random_state)):\n",
    "      \n",
    "    scaler = FTStandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_dev = scaler.transform(X_dev)\n",
    "    \n",
    "    batch_size = X_train.shape[1]\n",
    "    model = FTMultilayerPerceptron(nn_dimensions,\\\n",
    "        batch_size=batch_size,\\\n",
    "        random_state=random_state,\\\n",
    "        hidden_activation='tanh',\\\n",
    "        early_stopping=True,\\\n",
    "        patience=10,\\\n",
    "        max_epoch=100000,\\\n",
    "        verbose=10000)\n",
    "    \n",
    "    model.fit(X_train, y_train, X_dev=X_dev, y_dev=y_dev)\n",
    "    \n",
    "    cost_train = cross_entropy_cost(y_train, model.predict_probas(X_train))\n",
    "    mean_cost_train += cost_train\n",
    "    cost_dev = cross_entropy_cost(y_dev, model.predict_probas(X_dev))\n",
    "    mean_cost_dev += cost_dev\n",
    "    \n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_dev = model.predict(X_dev)\n",
    "    y_truth_train = one_hot_decoder(y_train)\n",
    "    y_truth_dev = one_hot_decoder(y_dev)\n",
    "    accuracy_train = accuracy(y_truth_train, y_pred_train)\n",
    "    mean_accuracy_train += accuracy_train\n",
    "    accuracy_dev = accuracy(y_truth_dev, y_pred_dev)\n",
    "    mean_accuracy_dev += accuracy_dev\n",
    "    \n",
    "    print()\n",
    "    print('Fold number ' + str(i))\n",
    "    print('Cost for training set = ' + str(cost_train))\n",
    "    print('Cost for      dev set = ' + str(cost_dev))\n",
    "    print()\n",
    "    print('Accuracy for training set = ' + str(accuracy_train))\n",
    "    print('Accuracy for      dev set = ' + str(accuracy_dev))\n",
    "    print()\n",
    "\n",
    "    \n",
    "mean_cost_train /= k\n",
    "mean_cost_dev /= k\n",
    "mean_accuracy_train /= k\n",
    "mean_accuracy_dev /= k\n",
    "\n",
    "print()\n",
    "print()\n",
    "print('Mean Cost for training set = ' + str(mean_cost_train))\n",
    "print('Mean Cost for      dev set = ' + str(mean_cost_dev))\n",
    "print()\n",
    "print('Mean Accuracy for training set = ' + str(mean_accuracy_train))\n",
    "print('Mean Accuracy for      dev set = ' + str(mean_accuracy_dev))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase hidden units again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_dimensions = [X_train.shape[0], 20, 20, 20, y_train.shape[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/100000 - loss: 1.3087060935571988 - val_loss: 1.3250277327849596\n",
      "End of training:\n",
      "epoch 7082/100000 - loss: 0.020565901405728185 - val_loss: 0.013994870723874828\n",
      "\n",
      "Fold number 0\n",
      "Cost for training set = 0.020561929058845584\n",
      "Cost for      dev set = 0.013994870723874828\n",
      "\n",
      "Accuracy for training set = 0.9941520467836257\n",
      "Accuracy for      dev set = 1.0\n",
      "\n",
      "epoch 0/100000 - loss: 1.3189109578000702 - val_loss: 1.1716282396410513\n",
      "End of training:\n",
      "epoch 1810/100000 - loss: 0.04707681283898881 - val_loss: 0.08519697145340348\n",
      "\n",
      "Fold number 1\n",
      "Cost for training set = 0.04706773893579337\n",
      "Cost for      dev set = 0.08519697145340348\n",
      "\n",
      "Accuracy for training set = 0.9902534113060428\n",
      "Accuracy for      dev set = 0.9821428571428571\n",
      "\n",
      "epoch 0/100000 - loss: 1.3162084476832179 - val_loss: 1.2108374028498368\n",
      "End of training:\n",
      "epoch 669/100000 - loss: 0.06513391387040354 - val_loss: 0.07276304302681981\n",
      "\n",
      "Fold number 2\n",
      "Cost for training set = 0.06510060802424608\n",
      "Cost for      dev set = 0.07276304302681981\n",
      "\n",
      "Accuracy for training set = 0.9805068226120858\n",
      "Accuracy for      dev set = 0.9821428571428571\n",
      "\n",
      "epoch 0/100000 - loss: 1.328953445105873 - val_loss: 1.0757633397953545\n",
      "End of training:\n",
      "epoch 836/100000 - loss: 0.0643757065588964 - val_loss: 0.028730505687836555\n",
      "\n",
      "Fold number 3\n",
      "Cost for training set = 0.0643528062675829\n",
      "Cost for      dev set = 0.028730505687836555\n",
      "\n",
      "Accuracy for training set = 0.9805068226120858\n",
      "Accuracy for      dev set = 0.9821428571428571\n",
      "\n",
      "epoch 0/100000 - loss: 1.3158347808275621 - val_loss: 1.285931344310328\n",
      "End of training:\n",
      "epoch 356/100000 - loss: 0.07662627144303125 - val_loss: 0.12197391160355903\n",
      "\n",
      "Fold number 4\n",
      "Cost for training set = 0.07651210380372374\n",
      "Cost for      dev set = 0.12197391160355903\n",
      "\n",
      "Accuracy for training set = 0.9805068226120858\n",
      "Accuracy for      dev set = 0.9464285714285714\n",
      "\n",
      "epoch 0/100000 - loss: 1.316860367037692 - val_loss: 1.1697585780984439\n",
      "epoch 10000/100000 - loss: 0.011437864515494764 - val_loss: 0.007400327485237816\n",
      "End of training:\n",
      "epoch 10742/100000 - loss: 0.009940048729692808 - val_loss: 0.007336953283221402\n",
      "\n",
      "Fold number 5\n",
      "Cost for training set = 0.00993815813921673\n",
      "Cost for      dev set = 0.007336953283221402\n",
      "\n",
      "Accuracy for training set = 0.9980506822612085\n",
      "Accuracy for      dev set = 1.0\n",
      "\n",
      "epoch 0/100000 - loss: 1.3168422584773352 - val_loss: 1.2334782215716131\n",
      "End of training:\n",
      "epoch 180/100000 - loss: 0.10675889521640983 - val_loss: 0.21385252383062062\n",
      "\n",
      "Fold number 6\n",
      "Cost for training set = 0.10629099829837776\n",
      "Cost for      dev set = 0.21385252383062062\n",
      "\n",
      "Accuracy for training set = 0.9668615984405458\n",
      "Accuracy for      dev set = 0.9285714285714286\n",
      "\n",
      "epoch 0/100000 - loss: 1.3114389682922913 - val_loss: 1.2547603904648226\n",
      "End of training:\n",
      "epoch 787/100000 - loss: 0.062256379333794847 - val_loss: 0.059868591643260256\n",
      "\n",
      "Fold number 7\n",
      "Cost for training set = 0.06223000058254804\n",
      "Cost for      dev set = 0.059868591643260256\n",
      "\n",
      "Accuracy for training set = 0.98635477582846\n",
      "Accuracy for      dev set = 0.9821428571428571\n",
      "\n",
      "epoch 0/100000 - loss: 1.3100265745903679 - val_loss: 1.2973409766143884\n",
      "End of training:\n",
      "epoch 4077/100000 - loss: 0.03386880899747065 - val_loss: 0.011662497931763488\n",
      "\n",
      "Fold number 8\n",
      "Cost for training set = 0.0338627132826149\n",
      "Cost for      dev set = 0.011662497931763488\n",
      "\n",
      "Accuracy for training set = 0.9902534113060428\n",
      "Accuracy for      dev set = 1.0\n",
      "\n",
      "epoch 0/100000 - loss: 1.3115717621856577 - val_loss: 1.2414202699119323\n",
      "epoch 10000/100000 - loss: 0.011139740276263677 - val_loss: 0.014502842865667867\n",
      "epoch 20000/100000 - loss: 0.0019050805266022316 - val_loss: 0.008788144964788374\n",
      "epoch 30000/100000 - loss: 0.0007371296175619922 - val_loss: 0.0067036908386038305\n",
      "epoch 40000/100000 - loss: 0.00041995133703589263 - val_loss: 0.005766291143815787\n",
      "epoch 50000/100000 - loss: 0.00028311112309166783 - val_loss: 0.005247043610398017\n",
      "epoch 60000/100000 - loss: 0.00020959702202797643 - val_loss: 0.00492182727015407\n",
      "epoch 70000/100000 - loss: 0.00016460329571910362 - val_loss: 0.004700854046417595\n",
      "epoch 80000/100000 - loss: 0.0001345793505405365 - val_loss: 0.004541472880725943\n",
      "epoch 90000/100000 - loss: 0.00011327896463718343 - val_loss: 0.004421083709148512\n",
      "End of training:\n",
      "epoch 100000/100000 - loss: 9.746488349306086e-05 - val_loss: 0.00432674010302111\n",
      "\n",
      "Fold number 9\n",
      "Cost for training set = 9.746350762050459e-05\n",
      "Cost for      dev set = 0.00432674010302111\n",
      "\n",
      "Accuracy for training set = 1.0\n",
      "Accuracy for      dev set = 1.0\n",
      "\n",
      "\n",
      "\n",
      "Mean Cost for training set = 0.048601451990056964\n",
      "Mean Cost for      dev set = 0.06197066092873806\n",
      "\n",
      "Mean Accuracy for training set = 0.9867446393762183\n",
      "Mean Accuracy for      dev set = 0.9803571428571429\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mean_cost_train = 0\n",
    "mean_cost_dev = 0\n",
    "mean_accuracy_train = 0\n",
    "mean_accuracy_dev = 0\n",
    "\n",
    "for i, (X_train, X_dev, y_train, y_dev) in enumerate(KFold(X, y, k, random_state=random_state)):\n",
    "      \n",
    "    scaler = FTStandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_dev = scaler.transform(X_dev)\n",
    "    \n",
    "    batch_size = X_train.shape[1]\n",
    "    model = FTMultilayerPerceptron(nn_dimensions,\\\n",
    "        batch_size=batch_size,\\\n",
    "        random_state=random_state,\\\n",
    "        hidden_activation='tanh',\\\n",
    "        early_stopping=True,\\\n",
    "        patience=10,\\\n",
    "        max_epoch=100000,\\\n",
    "        verbose=10000)\n",
    "    \n",
    "    model.fit(X_train, y_train, X_dev=X_dev, y_dev=y_dev)\n",
    "    \n",
    "    cost_train = cross_entropy_cost(y_train, model.predict_probas(X_train))\n",
    "    mean_cost_train += cost_train\n",
    "    cost_dev = cross_entropy_cost(y_dev, model.predict_probas(X_dev))\n",
    "    mean_cost_dev += cost_dev\n",
    "    \n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_dev = model.predict(X_dev)\n",
    "    y_truth_train = one_hot_decoder(y_train)\n",
    "    y_truth_dev = one_hot_decoder(y_dev)\n",
    "    accuracy_train = accuracy(y_truth_train, y_pred_train)\n",
    "    mean_accuracy_train += accuracy_train\n",
    "    accuracy_dev = accuracy(y_truth_dev, y_pred_dev)\n",
    "    mean_accuracy_dev += accuracy_dev\n",
    "    \n",
    "    print()\n",
    "    print('Fold number ' + str(i))\n",
    "    print('Cost for training set = ' + str(cost_train))\n",
    "    print('Cost for      dev set = ' + str(cost_dev))\n",
    "    print()\n",
    "    print('Accuracy for training set = ' + str(accuracy_train))\n",
    "    print('Accuracy for      dev set = ' + str(accuracy_dev))\n",
    "    print()\n",
    "\n",
    "    \n",
    "mean_cost_train /= k\n",
    "mean_cost_dev /= k\n",
    "mean_accuracy_train /= k\n",
    "mean_accuracy_dev /= k\n",
    "\n",
    "print()\n",
    "print()\n",
    "print('Mean Cost for training set = ' + str(mean_cost_train))\n",
    "print('Mean Cost for      dev set = ' + str(mean_cost_dev))\n",
    "print()\n",
    "print('Mean Accuracy for training set = ' + str(mean_accuracy_train))\n",
    "print('Mean Accuracy for      dev set = ' + str(mean_accuracy_dev))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decrease the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/100000 - loss: 1.4823778927256268 - val_loss: 2.915591106307399\n",
      "epoch 10000/100000 - loss: 0.1280554374755036 - val_loss: 1.0394564777252044\n",
      "epoch 20000/100000 - loss: 0.08202224691048102 - val_loss: 0.7675514429519823\n",
      "epoch 30000/100000 - loss: 0.060003219165110346 - val_loss: 0.5836052474810315\n",
      "epoch 40000/100000 - loss: 0.0480565646002832 - val_loss: 0.4708006304279736\n",
      "epoch 50000/100000 - loss: 0.041467649520011605 - val_loss: 0.403730996332112\n",
      "epoch 60000/100000 - loss: 0.0377881127938186 - val_loss: 0.3639659038362708\n",
      "epoch 70000/100000 - loss: 0.03570308137539638 - val_loss: 0.340831747302833\n",
      "epoch 80000/100000 - loss: 0.034496490935978355 - val_loss: 0.3276319850277055\n",
      "epoch 90000/100000 - loss: 0.03377616268746275 - val_loss: 0.32015365024608544\n",
      "End of training:\n",
      "epoch 100000/100000 - loss: 0.033326473566232674 - val_loss: 0.31572503239113653\n",
      "\n",
      "Fold number 0\n",
      "Cost for training set = 0.0052161816581970605\n",
      "Cost for      dev set = 0.0582150090564561\n",
      "\n",
      "Accuracy for training set = 1.0\n",
      "Accuracy for      dev set = 0.9821428571428571\n",
      "\n",
      "epoch 0/100000 - loss: 1.4925827569684982 - val_loss: 2.762193177203342\n",
      "epoch 10000/100000 - loss: 0.126061315440495 - val_loss: 1.105215468067969\n",
      "epoch 20000/100000 - loss: 0.0814108906452163 - val_loss: 0.819284148005809\n",
      "epoch 30000/100000 - loss: 0.05945021460908907 - val_loss: 0.6276924141698026\n",
      "epoch 40000/100000 - loss: 0.047431811878881625 - val_loss: 0.49950618554411774\n",
      "epoch 50000/100000 - loss: 0.0407486127118974 - val_loss: 0.42252499327907095\n",
      "epoch 60000/100000 - loss: 0.03698049958779837 - val_loss: 0.3757413044225531\n",
      "epoch 70000/100000 - loss: 0.03484342118039252 - val_loss: 0.34681331139167754\n",
      "epoch 80000/100000 - loss: 0.03359246622652536 - val_loss: 0.3292524067761695\n",
      "epoch 90000/100000 - loss: 0.03282803062835526 - val_loss: 0.317155333381433\n"
     ]
    }
   ],
   "source": [
    "mean_cost_train = 0\n",
    "mean_cost_dev = 0\n",
    "mean_accuracy_train = 0\n",
    "mean_accuracy_dev = 0\n",
    "\n",
    "for i, (X_train, X_dev, y_train, y_dev) in enumerate(KFold(X, y, k, random_state=random_state)):\n",
    "      \n",
    "    scaler = FTStandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_dev = scaler.transform(X_dev)\n",
    "    \n",
    "    batch_size = X_train.shape[1]\n",
    "    model = FTMultilayerPerceptron(nn_dimensions,\\\n",
    "        batch_size=batch_size,\\\n",
    "        random_state=random_state,\\\n",
    "        hidden_activation='tanh',\\\n",
    "        early_stopping=True,\\\n",
    "        l2_reg = True,\\\n",
    "        lambd=1.5,\\\n",
    "        patience=10,\\\n",
    "        max_epoch=100000,\\\n",
    "        verbose=10000)\n",
    "    \n",
    "    model.fit(X_train, y_train, X_dev=X_dev, y_dev=y_dev)\n",
    "    \n",
    "    cost_train = cross_entropy_cost(y_train, model.predict_probas(X_train))\n",
    "    mean_cost_train += cost_train\n",
    "    cost_dev = cross_entropy_cost(y_dev, model.predict_probas(X_dev))\n",
    "    mean_cost_dev += cost_dev\n",
    "    \n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_dev = model.predict(X_dev)\n",
    "    y_truth_train = one_hot_decoder(y_train)\n",
    "    y_truth_dev = one_hot_decoder(y_dev)\n",
    "    accuracy_train = accuracy(y_truth_train, y_pred_train)\n",
    "    mean_accuracy_train += accuracy_train\n",
    "    accuracy_dev = accuracy(y_truth_dev, y_pred_dev)\n",
    "    mean_accuracy_dev += accuracy_dev\n",
    "    \n",
    "    print()\n",
    "    print('Fold number ' + str(i))\n",
    "    print('Cost for training set = ' + str(cost_train))\n",
    "    print('Cost for      dev set = ' + str(cost_dev))\n",
    "    print()\n",
    "    print('Accuracy for training set = ' + str(accuracy_train))\n",
    "    print('Accuracy for      dev set = ' + str(accuracy_dev))\n",
    "    print()\n",
    "\n",
    "    \n",
    "mean_cost_train /= k\n",
    "mean_cost_dev /= k\n",
    "mean_accuracy_train /= k\n",
    "mean_accuracy_dev /= k\n",
    "\n",
    "print()\n",
    "print()\n",
    "print('Mean Cost for training set = ' + str(mean_cost_train))\n",
    "print('Mean Cost for      dev set = ' + str(mean_cost_dev))\n",
    "print()\n",
    "print('Mean Accuracy for training set = ' + str(mean_accuracy_train))\n",
    "print('Mean Accuracy for      dev set = ' + str(mean_accuracy_dev))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_dimensions = [X_train.shape[0], 20, 20, 20, y_train.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cost_train = 0\n",
    "mean_cost_dev = 0\n",
    "mean_accuracy_train = 0\n",
    "mean_accuracy_dev = 0\n",
    "\n",
    "for i, (X_train, X_dev, y_train, y_dev) in enumerate(KFold(X, y, k, random_state=random_state)):\n",
    "      \n",
    "    scaler = FTStandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_dev = scaler.transform(X_dev)\n",
    "    \n",
    "    batch_size = X_train.shape[1]\n",
    "    model = FTMultilayerPerceptron(nn_dimensions,\\\n",
    "        batch_size=batch_size,\\\n",
    "        random_state=random_state,\\\n",
    "        hidden_activation='tanh',\\\n",
    "        optimizer='adam',\\\n",
    "        early_stopping=True,\\\n",
    "        patience=10,\\\n",
    "        max_epoch=100000,\\\n",
    "        verbose=10000)\n",
    "    \n",
    "    model.fit(X_train, y_train, X_dev=X_dev, y_dev=y_dev)\n",
    "    \n",
    "    cost_train = cross_entropy_cost(y_train, model.predict_probas(X_train))\n",
    "    mean_cost_train += cost_train\n",
    "    cost_dev = cross_entropy_cost(y_dev, model.predict_probas(X_dev))\n",
    "    mean_cost_dev += cost_dev\n",
    "    \n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_dev = model.predict(X_dev)\n",
    "    y_truth_train = one_hot_decoder(y_train)\n",
    "    y_truth_dev = one_hot_decoder(y_dev)\n",
    "    accuracy_train = accuracy(y_truth_train, y_pred_train)\n",
    "    mean_accuracy_train += accuracy_train\n",
    "    accuracy_dev = accuracy(y_truth_dev, y_pred_dev)\n",
    "    mean_accuracy_dev += accuracy_dev\n",
    "    \n",
    "    print()\n",
    "    print('Fold number ' + str(i))\n",
    "    print('Cost for training set = ' + str(cost_train))\n",
    "    print('Cost for      dev set = ' + str(cost_dev))\n",
    "    print()\n",
    "    print('Accuracy for training set = ' + str(accuracy_train))\n",
    "    print('Accuracy for      dev set = ' + str(accuracy_dev))\n",
    "    print()\n",
    "\n",
    "    \n",
    "mean_cost_train /= k\n",
    "mean_cost_dev /= k\n",
    "mean_accuracy_train /= k\n",
    "mean_accuracy_dev /= k\n",
    "\n",
    "print()\n",
    "print()\n",
    "print('Mean Cost for training set = ' + str(mean_cost_train))\n",
    "print('Mean Cost for      dev set = ' + str(mean_cost_dev))\n",
    "print()\n",
    "print('Mean Accuracy for training set = ' + str(mean_accuracy_train))\n",
    "print('Mean Accuracy for      dev set = ' + str(mean_accuracy_dev))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
